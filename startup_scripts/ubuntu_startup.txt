Content-Type: multipart/mixed; boundary="//"
MIME-Version: 1.0

--//
Content-Type: text/cloud-config; charset="us-ascii"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment; filename="cloud-config.txt"

#cloud-config
cloud_final_modules:
- [scripts-user, always]

--//
Content-Type: text/x-shellscript; charset="us-ascii"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment; filename="userdata.txt"

#!/bin/bash
cd /home/ubuntu/

Update and install Docker as root
apt-get update
apt-get install --reinstall docker.io -y
apt-get install -y docker-compose
docker compose version

# Switch to ubuntu user and run the following commands
sudo -u ubuntu -i bash << 'EOF'
# Download and install Miniconda
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /home/ubuntu/Miniconda3-latest-Linux-x86_64.sh
bash /home/ubuntu/Miniconda3-latest-Linux-x86_64.sh -b -p /home/ubuntu/miniconda3
rm /home/ubuntu/Miniconda3-latest-Linux-x86_64.sh

# Initialize conda for bash shell
/home/ubuntu/miniconda3/bin/conda init

# Create a new conda environment
/home/ubuntu/miniconda3/bin/conda create --name fmbench_python311 -y python=3.11 ipykernel
# Activate the environment and install fmbench
source /home/ubuntu/miniconda3/bin/activate fmbench_python311
pip install -U fmbench

sudo usermod -a -G docker $USER
newgrp docker

# Download content from S3 using the provided script
curl -s https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/copy_s3_content.sh | sh -s -- /tmp
echo "__HF_TOKEN__" > /tmp/fmbench-read/scripts/hf_token.txt

# Add the conda environment activation and directory navigation to .bashrc
echo 'source /home/ubuntu/miniconda3/bin/activate fmbench_python311' >> /home/ubuntu/.bashrc


neuron="__neuron__"

# Check if neuron_ls is successful and neuron flag is set to True
if neuron-ls && [ "$neuron" = "True" ]; then
    # Download the Dockerfile for Triton
    curl -o ./Dockerfile_triton https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/src/fmbench/scripts/triton/Dockerfile_triton

    # Download the script that builds and pushes the Triton image locally
    curl -o build_and_push_triton.sh https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/src/fmbench/scripts/triton/build_and_push_triton.sh

    # Make the build and push script executable
    chmod +x build_and_push_triton.sh

    # Run the build and push script
    ./build_and_push_triton.sh
fi


# install triton inference server, install only if running on a GPU instance
if nvidia-smi; then
    cd ~
    git clone https://github.com/triton-inference-server/tensorrtllm_backend.git --branch v0.12.0
    # Update the submodules
    cd tensorrtllm_backend
    # Install git-lfs if needed
    sudo apt --fix-broken install -y
    sudo apt-get update -y && sudo apt-get install git-lfs -y --no-install-recommends
    git lfs install
    git submodule update --init --recursive
fi

source /home/ubuntu/.bashrc
touch /tmp/startup_complete.flag

EOF
